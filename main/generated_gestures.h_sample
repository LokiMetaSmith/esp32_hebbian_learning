/**
 * @file generated_gestures.h.sample
 * @brief Sample of the gesture library and graph data structures.
 *
 * @note THIS IS A SAMPLE FILE. DO NOT EDIT.
 * The actual `generated_gestures.h` file is generated by a Python script
 * based on data collected from a specific robot. It should not be manually
 * created or modified.
 *
 * --- How to Generate This File ---
 *
 * This file is generated by the training pipeline script located at:
 *   `tools/training_pipeline.py`
 *
 * The pipeline script connects to the robot, collects trajectory data,
 * clusters the data to form "gestures", and then exports the final
 * gesture library and transition graph into `main/generated_gestures.h`.
 *
 * For detailed instructions on how to set up the environment and run the
 * training pipeline, please refer to the main agent instructions file:
 *   `AGENTS.md`
 *
 * --- Data Structure Format ---
 *
 * The file defines two main data structures:
 *
 * 1. `g_gesture_library`: An array of `GestureToken` structs. Each token
 *    represents a single learned motion primitive.
 *
 * 2. `g_gesture_graph`: A `GestureGraph` struct that contains the complete
 *    gesture library and the costs of transitioning between any two gestures.
 */

#ifndef GENERATED_GESTURES_H
#define GENERATED_GESTURES_H

#include "planner.h" // Contains the definitions for GestureToken and GestureGraph

/**
 * @brief The library of all learned gesture tokens.
 * This is an array of GestureToken structs.
 */
static GestureToken g_gesture_library[] = {
  {
    // --- Gesture Token 0 ---
    .id = 0,                                  // Unique ID for this gesture
    .num_waypoints = 2,                       // Number of waypoints in this gesture's trajectory
    .energy_cost = 1.2345f,                   // Pre-calculated energy cost for executing this gesture
    .embedding = {0.1f, 0.2f, ...},           // The embedding vector for this gesture in the latent space
    .waypoints = {
      // Each waypoint defines a point in the trajectory.
      // The format depends on the robot type (e.g., arm vs. omni-base).
      // For an arm, it might be: {{pos_servo1, pos_servo2, ...}, {vel_servo1, vel_servo2, ...}}
      { // Waypoint 0
        {0.1f, 0.2f, 0.3f, 0.4f, 0.5f, 0.6f}, // Target positions
        {0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f}  // Target velocities
      },
      { // Waypoint 1
        {0.2f, 0.3f, 0.4f, 0.5f, 0.6f, 0.7f}, // Target positions
        {0.1f, 0.1f, 0.1f, 0.1f, 0.1f, 0.1f}  // Target velocities
      },
    }
  },
  // ... more GestureTokens would be listed here
};

/**
 * @brief The complete gesture graph.
 * Contains the gesture library and the transition costs between gestures.
 */
static GestureGraph g_gesture_graph = {
  .num_tokens = 1, // The total number of tokens in g_gesture_library
  .transition_costs = {
    // An adjacency matrix of transition costs (e.g., Euclidean distance between end/start poses).
    // cost[i][j] is the cost of moving from gesture i to gesture j.
    {0.0f, 2.5f, ...}, // Costs from gesture 0 to all others
    {2.5f, 0.0f, ...}, // Costs from gesture 1 to all others
    // ...
  },
  // This points to the gesture library defined above.
  .gesture_library = g_gesture_library
};

#endif // GENERATED_GESTURES_H
